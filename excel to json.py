# ğŸš— Ø±Ø¨Ø· Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ğŸ“¦ ØªØ«Ø¨ÙŠØª ijson
!pip install -q ijson

# ğŸ“š Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
import gzip, ijson, re, csv, os, glob, sys
from html import unescape
from pathlib import Path
from time import perf_counter

# =========================
# ğŸ”§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# =========================
JSON_DIR = "/content/drive/MyDrive/Ø¬ÙˆÙ…ÙŠØ§ Ø§Ù†Ø¬Ù„Ø´ Ù…Ø­ØªÙˆÙŠ"
JSON_PATH = None  # Ø£Ùˆ Ø­Ø¯Ø¯ÙŠ Ù…Ø³Ø§Ø± Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ù‡Ù†Ø§
CONTENT_LANG = "en"
OUT_DIR = "/content/drive/MyDrive/--JUMIA CONTENT/English content"
CHUNK_ROWS = 50_000
PROGRESS_EVERY = 100_000

# =========================
# ğŸ§½ ØªÙ†Ø¸ÙŠÙ HTML
# =========================
TAG_RE = re.compile(r"<[^>]+>")
def strip_html(s):
    return "" if s is None else unescape(TAG_RE.sub("", str(s)))

# =========================
# ğŸ§  Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# =========================
def get_lang_block(p, lang):
    return (p.get("contents") or {}).get(lang) or {}

def get_product_uid(p, lang):
    blk = get_lang_block(p, lang)
    return blk.get("product_uid") or p.get("uid") or ""

def get_all_image_urls_joined(p, lang, sep=" | "):
    blk = get_lang_block(p, lang)
    pd = (blk.get("product_details") or {})
    imgs = pd.get("image_url")
    urls = []
    if isinstance(imgs, list):
        urls = [img.strip() for img in imgs if isinstance(img, str) and img.strip()]
    elif isinstance(imgs, str) and imgs.strip():
        urls = [imgs.strip()]
    return sep.join(urls)

def get_variations(lang_block, uid_str):
    for el in (lang_block.get("elements") or []):
        if str(el.get("uid")) == str(uid_str):
            return [v.get("value") for v in (el.get("variations") or []) if v.get("value")]
    return []

def join(vals, nohtml=False):
    if not vals: return ""
    return " | ".join(strip_html(v) if nohtml else str(v) for v in vals)

# =========================
# ğŸ§¾ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# =========================
HEADERS = [
    "product_uid", "image_url", "urls",
    "page_title", "title",
    "description_html", "description",
    "specifications_html", "specifications",
    "img_alt", "meta_title", "meta_description"
]

# =========================
# ğŸ“ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙƒØªØ§Ø¨Ø© Ù„Ù„Ù€ CSV
# =========================
Path(OUT_DIR).mkdir(parents=True, exist_ok=True)

def new_writer(idx):
    path = os.path.join(OUT_DIR, f"{CONTENT_LANG}_part_{idx:05d}.csv")
    f = open(path, "w", encoding="utf-8-sig", newline="")  # Ù„ØªÙˆØ§ÙÙ‚ Excel
    w = csv.DictWriter(f, fieldnames=HEADERS)
    w.writeheader()
    return f, w, path

def process_stream(src_iter, writer_state):
    f, writer, current_path, totals = writer_state
    rows_in_chunk, total_rows, chunk_idx, start = totals

    for i, product in enumerate(src_iter, start=1):
        try:
            blk = get_lang_block(product, CONTENT_LANG)
            row = {
                "product_uid": get_product_uid(product, CONTENT_LANG),
                "image_url": get_all_image_urls_joined(product, CONTENT_LANG),
                "urls": " | ".join(product.get("urls", [])),
                "page_title":         join(get_variations(blk, "1")),
                "title":              join(get_variations(blk, "2")),
                "description_html":   join(get_variations(blk, "3")),
                "description":        join(get_variations(blk, "3"), nohtml=True),
                "specifications_html":join(get_variations(blk, "4")),
                "specifications":     join(get_variations(blk, "4"), nohtml=True),
                "img_alt":            join(get_variations(blk, "10")),
                "meta_title":         join(get_variations(blk, "100")),
                "meta_description":   join(get_variations(blk, "101")),
            }

            writer.writerow(row)
            rows_in_chunk += 1
            total_rows += 1

            if rows_in_chunk >= CHUNK_ROWS:
                f.close()
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {rows_in_chunk} ØµÙ â†’ {current_path}")
                chunk_idx += 1
                rows_in_chunk = 0
                f, writer, current_path = new_writer(chunk_idx)

            if total_rows % PROGRESS_EVERY == 0:
                elapsed = perf_counter() - start
                print(f"â±ï¸ ØªÙ‚Ø¯Ù‘Ù… ÙƒÙ„ÙŠ: {total_rows:,} ØµÙ â€¢ Ø²Ù…Ù†: {elapsed/60:.1f} Ø¯Ù‚ÙŠÙ‚Ø©")

        except Exception as e:
            print(f"âš ï¸ ØªØ®Ø·Ù‘ÙŠ ØµÙ Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£: {e}", file=sys.stderr)
            continue

    return (f, writer, current_path, (rows_in_chunk, total_rows, chunk_idx, start))

# =========================
# â–¶ï¸ Ø§Ù„ØªØ´ØºÙŠÙ„
# =========================
rows_in_chunk = 0
chunk_idx = 1
total_rows = 0
f, writer, current_path = new_writer(chunk_idx)
start = perf_counter()
print(f"ğŸš€ Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰: {OUT_DIR}")

if JSON_PATH:
    cands = [JSON_PATH]
else:
    cands = glob.glob(os.path.join(JSON_DIR, "*.json")) + glob.glob(os.path.join(JSON_DIR, "*.json.gz"))
    cands = sorted(cands)

if not cands:
    raise FileNotFoundError(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª JSON ÙÙŠ: {JSON_PATH or JSON_DIR}")

print(f"ğŸ“¦ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(cands)}")
for p in cands: print(" â€¢", p)

for file_idx, path in enumerate(cands, start=1):
    try:
        open_fn = gzip.open if str(path).lower().endswith(".gz") else open
        print(f"\nâ¡ï¸ Ø§Ù„Ù…Ù„Ù [{file_idx}/{len(cands)}]: {path}")
        with open_fn(path, "rt", encoding="utf-8") as src:
            parser = ijson.items(src, "item")
            f, writer, current_path, (rows_in_chunk, total_rows, chunk_idx, start) = process_stream(
                parser,
                (f, writer, current_path, (rows_in_chunk, total_rows, chunk_idx, start))
            )
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø¨Ø§Ù„Ù…Ù„Ù {path}: {e}", file=sys.stderr)
        continue

f.close()
elapsed = perf_counter() - start
print(f"\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡. Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {total_rows:,}")
print(f"ğŸ“ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {OUT_DIR}")
print(f"â³ Ø§Ù„Ø²Ù…Ù† Ø§Ù„ÙƒÙ„ÙŠ: {elapsed/60:.2f} Ø¯Ù‚ÙŠÙ‚Ø©")
